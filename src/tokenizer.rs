use crate::enums::token::{KEYWORD_MAP, Token};
use crate::utils::get_file_contents;

struct CharStream<'a> {
    chars: std::iter::Peekable<std::str::Chars<'a>>,
}

impl CharStream<'_> {
    fn peek(&mut self) -> Option<char> {
        self.chars.peek().copied()
    }

    fn next(&mut self) -> Option<char> {
        self.chars.next()
    }
}

pub fn tokenize(filename: &str) -> (Vec<Token>, Vec<Token>) {
    let mut output: Vec<Token> = Vec::new();
    let mut errors: Vec<Token> = Vec::new();

    let file_contents = get_file_contents(filename);

    if !file_contents.is_empty() {
        let chars = file_contents.chars().peekable();
        let mut tokens = CharStream { chars };

        let mut line_number = 1;

        while let Some(token) = tokens.next() {
            let token = match token {
                '(' => Token::LeftParen,
                ')' => Token::RightParen,
                '{' => Token::LeftBrace,
                '}' => Token::RightBrace,
                '.' => Token::Dot,
                ',' => Token::Comma,
                '+' => Token::Plus,
                '-' => Token::Minus,
                '*' => Token::Star,
                ';' => Token::SemiColon,
                '=' => get_equal_token(&mut tokens, Token::EqualEqual, Token::Equal),
                '!' => get_equal_token(&mut tokens, Token::BangEqual, Token::Bang),
                '>' => get_equal_token(&mut tokens, Token::GreaterEqual, Token::Greater),
                '<' => get_equal_token(&mut tokens, Token::LessEqual, Token::Less),
                '/' => match tokens.peek() {
                    Some('/') => {
                        tokens.next();
                        remove_comment(&mut tokens);
                        continue;
                    }
                    _ => Token::Division,
                },
                ' ' | '\t' => {
                    continue;
                }
                '\n' => {
                    line_number += 1;
                    continue;
                }
                '"' => get_string_token(&mut tokens, line_number),
                number if token.is_numeric() => get_numeric_token(&mut tokens, number, line_number),
                identifier if token.is_alphabetic() || token == '_' => {
                    get_identifier(&mut tokens, identifier)
                }
                _ => Token::Error(token, line_number),
            };

            match token {
                Token::ErrorString(_, _) | Token::Error(_, _) => {
                    errors.push(token);
                }
                _ => {
                    output.push(token);
                }
            }
        }
    }

    (output, errors)
}

fn remove_comment(tokens: &mut CharStream) {
    while let Some(next) = tokens.peek() {
        match next {
            '\n' => {
                return;
            }
            _ => {
                tokens.next();
            }
        }
    }
}

fn get_identifier(tokens: &mut CharStream, token: char) -> Token {
    let mut word = token.to_string();

    while let Some(next) = tokens.peek() {
        match next {
            char if next.is_alphanumeric() || next == '_' => {
                tokens.next();
                word.push(char);
            }
            _ => return identifier_or_keyword(&word),
        }
    }

    identifier_or_keyword(&word)
}

fn identifier_or_keyword(word: &str) -> Token {
    KEYWORD_MAP
        .get(word)
        .cloned()
        .unwrap_or_else(|| Token::Identifier(word.to_string()))
}

fn get_numeric_token(tokens: &mut CharStream, token: char, line: usize) -> Token {
    let mut number = token.to_string();
    let mut decimal = false;

    while let Some(next) = tokens.peek() {
        match next {
            '.' => {
                if decimal {
                    return Token::ErrorString(number, 1);
                }

                decimal = true;
                tokens.next();
                number.push(next);
            }
            c if next.is_numeric() => {
                tokens.next();
                number.push(c);
            }
            _ => return parse_number(number, line),
        }
    }

    parse_number(number, line)
}

fn parse_number(string: String, line: usize) -> Token {
    match string.parse::<f64>() {
        Ok(number) => Token::Number(string, number),
        Err(_) => Token::ErrorString(string, line),
    }
}

fn get_string_token(tokens: &mut CharStream, line: usize) -> Token {
    let mut string = String::new();
    while let Some(next) = tokens.peek() {
        match next {
            '"' => {
                tokens.next();
                return Token::String(string);
            }
            c => {
                tokens.next();
                string.push(c);
            }
        }
    }

    Token::ErrorString(string, line)
}

fn get_equal_token(tokens: &mut CharStream, if_equal_token: Token, token: Token) -> Token {
    match tokens.peek() {
        Some('=') => {
            tokens.next();
            if_equal_token
        }
        _ => token,
    }
}
